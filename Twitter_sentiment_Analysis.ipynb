{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Madhusowmya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"training.1600000.processed.noemoticon.csv\" , encoding= 'ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "polarity of tweet ",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id of the tweet",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "date of the tweet",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "query",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "user",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text of the tweet ",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "bb6f6401-8fae-47bf-926a-ecda250ea98d",
       "rows": [
        [
         "0",
         "0",
         "1467810672",
         "Mon Apr 06 22:19:49 PDT 2009",
         "NO_QUERY",
         "scotthamilton",
         "is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!"
        ],
        [
         "1",
         "0",
         "1467810917",
         "Mon Apr 06 22:19:53 PDT 2009",
         "NO_QUERY",
         "mattycus",
         "@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds"
        ],
        [
         "2",
         "0",
         "1467811184",
         "Mon Apr 06 22:19:57 PDT 2009",
         "NO_QUERY",
         "ElleCTF",
         "my whole body feels itchy and like its on fire "
        ],
        [
         "3",
         "0",
         "1467811193",
         "Mon Apr 06 22:19:57 PDT 2009",
         "NO_QUERY",
         "Karoli",
         "@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. "
        ],
        [
         "4",
         "0",
         "1467811372",
         "Mon Apr 06 22:20:00 PDT 2009",
         "NO_QUERY",
         "joy_wolf",
         "@Kwesidei not the whole crew "
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity of tweet</th>\n",
       "      <th>id of the tweet</th>\n",
       "      <th>date of the tweet</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text of the tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity of tweet   id of the tweet             date of the tweet  \\\n",
       "0                   0       1467810672  Mon Apr 06 22:19:49 PDT 2009   \n",
       "1                   0       1467810917  Mon Apr 06 22:19:53 PDT 2009   \n",
       "2                   0       1467811184  Mon Apr 06 22:19:57 PDT 2009   \n",
       "3                   0       1467811193  Mon Apr 06 22:19:57 PDT 2009   \n",
       "4                   0       1467811372  Mon Apr 06 22:20:00 PDT 2009   \n",
       "\n",
       "      query           user                                 text of the tweet   \n",
       "0  NO_QUERY  scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "1  NO_QUERY       mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "2  NO_QUERY        ElleCTF    my whole body feels itchy and like its on fire   \n",
       "3  NO_QUERY         Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "4  NO_QUERY       joy_wolf                      @Kwesidei not the whole crew   "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['target' , 'id' , 'date' , 'flag' , 'user' , 'text']\n",
    "dataset.columns = col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "target",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flag",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "user",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "9a732ecd-c338-4d9f-9019-676268c8c2b3",
       "rows": [
        [
         "0",
         "0",
         "1467810672",
         "Mon Apr 06 22:19:49 PDT 2009",
         "NO_QUERY",
         "scotthamilton",
         "is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!"
        ],
        [
         "1",
         "0",
         "1467810917",
         "Mon Apr 06 22:19:53 PDT 2009",
         "NO_QUERY",
         "mattycus",
         "@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds"
        ],
        [
         "2",
         "0",
         "1467811184",
         "Mon Apr 06 22:19:57 PDT 2009",
         "NO_QUERY",
         "ElleCTF",
         "my whole body feels itchy and like its on fire "
        ],
        [
         "3",
         "0",
         "1467811193",
         "Mon Apr 06 22:19:57 PDT 2009",
         "NO_QUERY",
         "Karoli",
         "@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. "
        ],
        [
         "4",
         "0",
         "1467811372",
         "Mon Apr 06 22:20:00 PDT 2009",
         "NO_QUERY",
         "joy_wolf",
         "@Kwesidei not the whole crew "
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag           user  \\\n",
       "0       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY  scotthamilton   \n",
       "1       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY       mattycus   \n",
       "2       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY        ElleCTF   \n",
       "3       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         Karoli   \n",
       "4       0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY       joy_wolf   \n",
       "\n",
       "                                                text  \n",
       "0  is upset that he can't update his Facebook by ...  \n",
       "1  @Kenichan I dived many times for the ball. Man...  \n",
       "2    my whole body feels itchy and like its on fire   \n",
       "3  @nationwideclass no, it's not behaving at all....  \n",
       "4                      @Kwesidei not the whole crew   "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1048572, 6)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target    0\n",
       "id        0\n",
       "date      0\n",
       "flag      0\n",
       "user      0\n",
       "text      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for missing values\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    799996\n",
       "4    248576\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribution of tweets\n",
    "dataset['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting 0 to -ve and 4 to +ve\n",
    "dataset['target'] = dataset['target'].map({0:0 , 4:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    799996\n",
       "1    248576\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "\n",
    "stremmer = PorterStemmer()\n",
    "\n",
    "def stemming(content):\n",
    "    stemmed_content = re.sub('[^a-zA-Z]',' ',content) # removing not a-z and A-Z\n",
    "    stemmed_content = stemmed_content.lower()\n",
    "    stemmed_content = stemmed_content.split()\n",
    "    stemmed_content = [stremmer.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n",
    "    stemmed_content = ' '.join(stemmed_content)\n",
    "    return stemmed_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['text'] = dataset['text'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "target",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flag",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "user",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "f8fdf583-dfcd-4522-8ae0-2e7f2971828f",
       "rows": [
        [
         "0",
         "0",
         "1467810672",
         "Mon Apr 06 22:19:49 PDT 2009",
         "NO_QUERY",
         "scotthamilton",
         "upset updat facebook text might cri result school today also blah"
        ],
        [
         "1",
         "0",
         "1467810917",
         "Mon Apr 06 22:19:53 PDT 2009",
         "NO_QUERY",
         "mattycus",
         "kenichan dive mani time ball manag save rest go bound"
        ],
        [
         "2",
         "0",
         "1467811184",
         "Mon Apr 06 22:19:57 PDT 2009",
         "NO_QUERY",
         "ElleCTF",
         "whole bodi feel itchi like fire"
        ],
        [
         "3",
         "0",
         "1467811193",
         "Mon Apr 06 22:19:57 PDT 2009",
         "NO_QUERY",
         "Karoli",
         "nationwideclass behav mad see"
        ],
        [
         "4",
         "0",
         "1467811372",
         "Mon Apr 06 22:20:00 PDT 2009",
         "NO_QUERY",
         "joy_wolf",
         "kwesidei whole crew"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>upset updat facebook text might cri result sch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>kenichan dive mani time ball manag save rest g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>whole bodi feel itchi like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>nationwideclass behav mad see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>kwesidei whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag           user  \\\n",
       "0       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY  scotthamilton   \n",
       "1       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY       mattycus   \n",
       "2       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY        ElleCTF   \n",
       "3       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         Karoli   \n",
       "4       0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY       joy_wolf   \n",
       "\n",
       "                                                text  \n",
       "0  upset updat facebook text might cri result sch...  \n",
       "1  kenichan dive mani time ball manag save rest g...  \n",
       "2                    whole bodi feel itchi like fire  \n",
       "3                      nationwideclass behav mad see  \n",
       "4                                kwesidei whole crew  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset['text']\n",
    "y = dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataset\n",
    "x_train , x_test , y_train , y_test = train_test_split(x , y , test_size = 0.2 , random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert textual data to numerical data\n",
    "vectorizer = TfidfVectorizer()\n",
    "x_train = vectorizer.fit_transform(x_train)\n",
    "x_test = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 6198292 stored elements and shape (838857, 328138)>\n",
      "  Coords\tValues\n",
      "  (0, 38891)\t0.400775143648158\n",
      "  (0, 14592)\t0.36261851892418073\n",
      "  (0, 310440)\t0.23944674945165806\n",
      "  (0, 213360)\t0.33741977246344723\n",
      "  (0, 106715)\t0.1976367684924913\n",
      "  (0, 78032)\t0.39706301209607425\n",
      "  (0, 238401)\t0.5830788261117192\n",
      "  (1, 310440)\t0.16815634344198596\n",
      "  (1, 234536)\t0.48130448708128454\n",
      "  (1, 317696)\t0.20514010986264616\n",
      "  (1, 271849)\t0.24304190125482247\n",
      "  (1, 94523)\t0.43038800147556905\n",
      "  (1, 122944)\t0.4035858443173664\n",
      "  (1, 198441)\t0.2870371557279857\n",
      "  (1, 8855)\t0.2240883171081635\n",
      "  (1, 68179)\t0.2864104053098128\n",
      "  (1, 224365)\t0.27598082539483954\n",
      "  (2, 308903)\t0.3260091681570522\n",
      "  (2, 215352)\t0.4044438546293875\n",
      "  (2, 277744)\t0.3822383090103777\n",
      "  (2, 35126)\t0.39464818697445025\n",
      "  (2, 121997)\t0.19090014442691997\n",
      "  (2, 291621)\t0.2875143207107189\n",
      "  (2, 55892)\t0.20543830173559866\n",
      "  (2, 277759)\t0.5166987795580361\n",
      "  :\t:\n",
      "  (838853, 37072)\t0.3073996032889281\n",
      "  (838853, 35293)\t0.35120282974728395\n",
      "  (838853, 83433)\t0.3529767040982573\n",
      "  (838853, 201755)\t0.43186659378280373\n",
      "  (838853, 175462)\t0.4843113142929819\n",
      "  (838854, 309938)\t0.2916398374107532\n",
      "  (838854, 108109)\t0.24653123457104892\n",
      "  (838854, 194230)\t0.3641738303097542\n",
      "  (838854, 13908)\t0.3276537199403908\n",
      "  (838854, 321487)\t0.3838609888694572\n",
      "  (838854, 132029)\t0.40871415687247525\n",
      "  (838854, 9893)\t0.5475331003134281\n",
      "  (838855, 89422)\t0.2796713365042031\n",
      "  (838855, 290970)\t0.19081652911078728\n",
      "  (838855, 54708)\t0.3156460233110709\n",
      "  (838855, 56420)\t0.29151237557524495\n",
      "  (838855, 284762)\t0.2783921225065403\n",
      "  (838855, 146414)\t0.3200424427763186\n",
      "  (838855, 126321)\t0.2870147273919559\n",
      "  (838855, 304105)\t0.2501809764029623\n",
      "  (838855, 165171)\t0.3188668557702709\n",
      "  (838855, 289138)\t0.5236293853059456\n",
      "  (838856, 284921)\t0.335072583279159\n",
      "  (838856, 20733)\t0.4318954964970969\n",
      "  (838856, 103826)\t0.8373724643431723\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Madhusowmya\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8331831294852537\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "y_pred = model.predict(x_test)\n",
    "print(accuracy_score(y_test , y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the sentiment\n",
    "def predict_sentiment(text):\n",
    "    text = re.sub('[^a-zA-Z]',' ',text) # removing not a-z and A-Z\n",
    "    text = text.lower()\n",
    "    text = text.split() \n",
    "    text = [stremmer.stem(word) for word in text if not word in stopwords.words('english')]\n",
    "    text = ' '.join(text)\n",
    "    text = [text]\n",
    "    text = vectorizer.transform(text)   \n",
    "    sentiment = model.predict(text)\n",
    "    if sentiment == 0:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "print(predict_sentiment(\"I hate you\"))\n",
    "print(predict_sentiment(\"I love you\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "import pickle\n",
    "pickle.dump(model , open('model.pkl' , 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vectorizer , open('vectorizer.pkl' , 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Madhusowmya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.8337\n",
      "Naive Bayes Accuracy: 0.7807\n",
      "Linear SVM Accuracy: 0.8296\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load dataset\n",
    "dataset = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding='ISO-8859-1')\n",
    "col_names = ['target', 'id', 'date', 'flag', 'user', 'text']\n",
    "dataset.columns = col_names\n",
    "\n",
    "# Keep only relevant columns\n",
    "dataset = dataset[['text', 'target']]\n",
    "\n",
    "# Map target values (0 = Negative, 4 = Positive)\n",
    "dataset['target'] = dataset['target'].map({0: 0, 4: 1})\n",
    "\n",
    "# Text preprocessing with stemming\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stemming(content):\n",
    "    content = re.sub('[^a-zA-Z]', ' ', content)\n",
    "    content = content.lower()\n",
    "    content = content.split()\n",
    "    content = [stemmer.stem(word) for word in content if word not in stop_words]\n",
    "    return ' '.join(content)\n",
    "\n",
    "dataset['text'] = dataset['text'].apply(stemming)\n",
    "\n",
    "# Split data\n",
    "x = dataset['text']\n",
    "y = dataset['target']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "x_train = vectorizer.fit_transform(x_train)\n",
    "x_test = vectorizer.transform(x_test)\n",
    "\n",
    "# Dictionary to store models and accuracies\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"Linear SVM\": LinearSVC(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "accuracies = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, clf in models.items():\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies[name] = accuracy\n",
    "    print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = max(accuracies, key=accuracies.get)\n",
    "print(f\"\\nBest Model: {best_model_name} with Accuracy: {accuracies[best_model_name]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Madhusowmya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\Madhusowmya\\AppData\\Local\\Temp\\ipykernel_22272\\626727311.py:21: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding='ISO-8859-1', header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Madhusowmya\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 20ms/step - accuracy: 0.6340 - loss: nan - val_accuracy: 0.6337 - val_loss: nan\n",
      "Epoch 2/100\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 20ms/step - accuracy: 0.6340 - loss: nan - val_accuracy: 0.6337 - val_loss: nan\n",
      "Epoch 3/100\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 25ms/step - accuracy: 0.6374 - loss: nan - val_accuracy: 0.6337 - val_loss: nan\n",
      "Epoch 4/100\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 34ms/step - accuracy: 0.6337 - loss: nan - val_accuracy: 0.6337 - val_loss: nan\n",
      "Epoch 5/100\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 21ms/step - accuracy: 0.6348 - loss: nan - val_accuracy: 0.6337 - val_loss: nan\n",
      "Epoch 6/100\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 20ms/step - accuracy: 0.6382 - loss: nan - val_accuracy: 0.6337 - val_loss: nan\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Madhusowmya\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 25ms/step - accuracy: 0.6332 - loss: nan - val_accuracy: 0.6337 - val_loss: nan\n",
      "Epoch 2/100\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 35ms/step - accuracy: 0.6369 - loss: nan - val_accuracy: 0.6337 - val_loss: nan\n",
      "Epoch 3/100\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 35ms/step - accuracy: 0.6381 - loss: nan - val_accuracy: 0.6337 - val_loss: nan\n",
      "Epoch 4/100\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 34ms/step - accuracy: 0.6326 - loss: nan - val_accuracy: 0.6337 - val_loss: nan\n",
      "Epoch 5/100\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 34ms/step - accuracy: 0.6355 - loss: nan - val_accuracy: 0.6337 - val_loss: nan\n",
      "Epoch 6/100\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 34ms/step - accuracy: 0.6329 - loss: nan - val_accuracy: 0.6337 - val_loss: nan\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Madhusowmya\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 45ms/step - accuracy: 0.6359 - loss: nan - val_accuracy: 0.6337 - val_loss: nan\n",
      "Epoch 2/100\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 45ms/step - accuracy: 0.6337 - loss: nan - val_accuracy: 0.6337 - val_loss: nan\n",
      "Epoch 3/100\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 43ms/step - accuracy: 0.6354 - loss: nan - val_accuracy: 0.6337 - val_loss: nan\n",
      "Epoch 4/100\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 43ms/step - accuracy: 0.6359 - loss: nan - val_accuracy: 0.6337 - val_loss: nan\n",
      "Epoch 5/100\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 43ms/step - accuracy: 0.6332 - loss: nan - val_accuracy: 0.6337 - val_loss: nan\n",
      "Epoch 6/100\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 42ms/step - accuracy: 0.6382 - loss: nan - val_accuracy: 0.6337 - val_loss: nan\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Madhusowmya\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - accuracy: 0.6332 - loss: nan - val_accuracy: 0.6337 - val_loss: nan\n",
      "Epoch 2/100\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - accuracy: 0.6347 - loss: nan - val_accuracy: 0.6337 - val_loss: nan\n",
      "Epoch 3/100\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - accuracy: 0.6307 - loss: nan - val_accuracy: 0.6337 - val_loss: nan\n",
      "Epoch 4/100\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 11ms/step - accuracy: 0.6370 - loss: nan - val_accuracy: 0.6337 - val_loss: nan\n",
      "Epoch 5/100\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - accuracy: 0.6336 - loss: nan - val_accuracy: 0.6337 - val_loss: nan\n",
      "Epoch 6/100\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - accuracy: 0.6370 - loss: nan - val_accuracy: 0.6337 - val_loss: nan\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Madhusowmya\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 22ms/step - accuracy: 0.6336 - loss: nan - val_accuracy: 0.6337 - val_loss: nan\n",
      "Epoch 2/100\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - accuracy: 0.6396 - loss: nan - val_accuracy: 0.6337 - val_loss: nan\n",
      "Epoch 3/100\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 14ms/step - accuracy: 0.6363 - loss: nan - val_accuracy: 0.6337 - val_loss: nan\n",
      "Epoch 4/100\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 15ms/step - accuracy: 0.6366 - loss: nan - val_accuracy: 0.6337 - val_loss: nan\n",
      "Epoch 5/100\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - accuracy: 0.6352 - loss: nan - val_accuracy: 0.6337 - val_loss: nan\n",
      "Epoch 6/100\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.6395 - loss: nan - val_accuracy: 0.6337 - val_loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Simple_LSTM', 0.633650004863739)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, GRU, Bidirectional, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load and preprocess data\n",
    "df = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding='ISO-8859-1', header=None)\n",
    "df.columns = ['target', 'id', 'date', 'flag', 'user', 'text']\n",
    "df = df[['text', 'target']]\n",
    "df['target'] = df['target'].map({0: 0, 4: 1})\n",
    "\n",
    "# Text cleaning function\n",
    "stemmer = PorterStemmer()\n",
    "def clean(text):\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text).lower().split()\n",
    "    return ' '.join([stemmer.stem(word) for word in text if word not in stop_words])\n",
    "df['text'] = df['text'].apply(clean)\n",
    "\n",
    "# Reduce dataset for speed\n",
    "df = df.sample(100000, random_state=42)\n",
    "\n",
    "x = df['text']\n",
    "y = df['target']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=100)\n",
    "x_test = pad_sequences(x_test, maxlen=100)\n",
    "\n",
    "# Model configurations\n",
    "dl_models = {}\n",
    "dl_accuracies = {}\n",
    "\n",
    "# Callback to stop training when validation accuracy stops improving\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "\n",
    "def compile_and_train(model, name):\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, epochs=100, batch_size=64, validation_data=(x_test, y_test), verbose=1, callbacks=[early_stopping])\n",
    "    acc = model.evaluate(x_test, y_test, verbose=0)[1]\n",
    "    dl_models[name] = model\n",
    "    dl_accuracies[name] = acc\n",
    "\n",
    "# Model 1: Simple LSTM\n",
    "model1 = Sequential([\n",
    "    Embedding(5000, 128, input_length=100),\n",
    "    LSTM(64),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "compile_and_train(model1, \"Simple_LSTM\")\n",
    "\n",
    "# Model 2: GRU\n",
    "model2 = Sequential([\n",
    "    Embedding(5000, 128, input_length=100),\n",
    "    GRU(64),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "compile_and_train(model2, \"GRU\")\n",
    "\n",
    "# Model 3: Bidirectional LSTM\n",
    "model3 = Sequential([\n",
    "    Embedding(5000, 128, input_length=100),\n",
    "    Bidirectional(LSTM(64)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "compile_and_train(model3, \"BiLSTM\")\n",
    "\n",
    "# Model 4: CNN\n",
    "model4 = Sequential([\n",
    "    Embedding(5000, 128, input_length=100),\n",
    "    Conv1D(128, 5, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "compile_and_train(model4, \"CNN\")\n",
    "\n",
    "# Model 5: CNN + LSTM\n",
    "model5 = Sequential([\n",
    "    Embedding(5000, 128, input_length=100),\n",
    "    Conv1D(64, 5, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    LSTM(64),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "compile_and_train(model5, \"CNN_LSTM\")\n",
    "\n",
    "# Find best model\n",
    "best_dl_model_name = max(dl_accuracies, key=dl_accuracies.get)\n",
    "best_dl_model = dl_models[best_dl_model_name]\n",
    "\n",
    "# Save best model and tokenizer\n",
    "best_dl_model.save(\"best_dl_model.h5\")\n",
    "with open(\"dl_tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "best_dl_model_name, dl_accuracies[best_dl_model_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "Negative\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load stopwords once to avoid calling stopwords.words repeatedly\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load your trained deep learning model\n",
    "dl_model = load_model('best_dl_model.h5')\n",
    "\n",
    "# Assuming the tokenizer is saved as 'dl_tokenizer.pkl'\n",
    "import pickle\n",
    "with open('dl_tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "def predict_sentiment_dl(text):\n",
    "    # Text preprocessing\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)  # Remove non-alphabetic characters\n",
    "    text = text.lower().split()  # Convert to lowercase and split into words\n",
    "    text = [stemmer.stem(word) for word in text if word not in stop_words]  # Stem and remove stopwords\n",
    "    \n",
    "    # Rejoin the list of words back into a string\n",
    "    text = ' '.join(text)\n",
    "    \n",
    "    # Vectorize the text using the tokenizer (you should already have 'tokenizer' trained during model training)\n",
    "    text_vectorized = tokenizer.texts_to_sequences([text])  # Convert text to sequence\n",
    "    text_vectorized = pad_sequences(text_vectorized, maxlen=100)  # Pad to match the input shape of the model\n",
    "    \n",
    "    # Predict sentiment using the DL model\n",
    "    prediction = dl_model.predict(text_vectorized)\n",
    "    \n",
    "    # If prediction is greater than or equal to 0.5, classify as Positive\n",
    "    return \"Positive\" if prediction >= 0.5 else \"Negative\"\n",
    "\n",
    "# Test the function\n",
    "print(predict_sentiment_dl(\"I love this project\"))\n",
    "print(predict_sentiment_dl(\"I hate waiting in lines\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Example model (you'll already have yours)\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(100,)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Save the model\n",
    "model.save(\"dl_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Cost = 0.8591621665974379\n",
      "Step 10: Cost = 0.7383211002771717\n",
      "Step 20: Cost = 0.702513372145349\n",
      "Step 30: Cost = 0.6902951135752973\n",
      "Step 40: Cost = 0.6815711635436168\n",
      "Step 50: Cost = 0.6776818342428622\n",
      "Step 60: Cost = 0.6738998258208582\n",
      "Step 70: Cost = 0.6711289740704949\n",
      "Step 80: Cost = 0.6684963868437728\n",
      "Step 90: Cost = 0.6662713125121701\n",
      "Step 100: Cost = 0.6643740567189699\n",
      "Step 110: Cost = 0.6627639826622537\n",
      "Step 120: Cost = 0.6614455270022898\n",
      "Step 130: Cost = 0.6604164104234838\n",
      "Step 140: Cost = 0.6596622998500052\n",
      "Step 150: Cost = 0.659143986903786\n",
      "Step 160: Cost = 0.65880224839487\n",
      "Step 170: Cost = 0.6585759367719106\n",
      "Step 180: Cost = 0.6584163522729028\n",
      "Step 190: Cost = 0.6582911563268883\n",
      "Optimized parameters: [ 0.27097343  0.44500062  0.76051767 -0.2559001   0.93190422  0.51005678\n",
      "  1.03540644 -0.02567562  0.25721029  0.18952773  0.46432666  0.29428221\n",
      "  0.16507352  1.08810032  0.51587984  0.09591538  1.38078517 -0.06961413\n",
      "  0.32231442  0.13947141  0.54933753  0.63848113  0.96558047  0.66583035]\n",
      "Optimized model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Define the quantum device\n",
    "n_qubits = 4  # Update this if you are using a different number\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "# Define a deeper quantum circuit\n",
    "@qml.qnode(dev)\n",
    "def quantum_circuit(params, x):\n",
    "    # Encode classical data into quantum states\n",
    "    for i in range(n_qubits):\n",
    "        qml.RY(x[i], wires=i)\n",
    "    \n",
    "    # Apply multiple layers of parameterized gates and entanglement\n",
    "    for layer in range(len(params) // (2 * n_qubits)):\n",
    "        for i in range(n_qubits):\n",
    "            qml.RY(params[layer * 2 * n_qubits + i], wires=i)\n",
    "            qml.RZ(params[layer * 2 * n_qubits + i + n_qubits], wires=i)\n",
    "        for i in range(n_qubits - 1):\n",
    "            qml.CNOT(wires=[i, i + 1])\n",
    "    \n",
    "    # Measure expectation value\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "# Update the cost function\n",
    "def cost(params, X, Y):\n",
    "    loss = 0\n",
    "    for x, y in zip(X, Y):\n",
    "        predictions = quantum_circuit(params, x)\n",
    "        loss += np.sum((predictions - y) ** 2)\n",
    "    return loss / len(X)\n",
    "\n",
    "# Increase the number of layers\n",
    "num_layers = 3\n",
    "params = qml.numpy.random.rand(num_layers * 2 * n_qubits, requires_grad=True)\n",
    "\n",
    "# Optimize the parameters\n",
    "opt = qml.AdamOptimizer(stepsize=0.01)\n",
    "for i in range(200):  # Increase the number of iterations\n",
    "    params = opt.step(lambda p: cost(p, x_train, y_train), params)\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Step {i}: Cost = {cost(params, x_train, y_train)}\")\n",
    "\n",
    "print(\"Optimized parameters:\", params)\n",
    "\n",
    "# Save the optimized model\n",
    "with open(\"optimized_quantum_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(params, f)\n",
    "\n",
    "print(\"Optimized model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_pad' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Preprocess Data\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X_pad, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Build and compile the QDL model\u001b[39;00m\n\u001b[0;32m     47\u001b[0m qdl_model \u001b[38;5;241m=\u001b[39m QDLModel(num_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_pad' is not defined"
     ]
    }
   ],
   "source": [
    "# QDL Hybrid Model: Quantum-Deep Learning Model Integration\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp\n",
    "import tensorflow as tf\n",
    "\n",
    "# Quantum layer\n",
    "def quantum_layer(inputs):\n",
    "    dev = qml.device(\"default.qubit\", wires=2)\n",
    "\n",
    "    @qml.qnode(dev)\n",
    "    def qnode(inputs, weights):\n",
    "        qml.templates.AngleEmbedding(inputs, wires=[0, 1])\n",
    "        qml.templates.BasicEntanglerLayers(weights, wires=[0, 1])\n",
    "        return qml.expval(qml.PauliZ(0))  # Output as expectation value of PauliZ\n",
    "    \n",
    "    weights = pnp.array([[0.1, 0.2], [0.3, 0.4]], requires_grad=True)\n",
    "    results = []\n",
    "    for sample in inputs:\n",
    "        result = qnode(sample, weights)\n",
    "        results.append(result)\n",
    "    return pnp.array(results)\n",
    "\n",
    "# QDL Model (Hybrid Model)\n",
    "class QDLModel(tf.keras.Model):\n",
    "    def __init__(self, num_features):\n",
    "        super(QDLModel, self).__init__()\n",
    "        self.embedding = layers.Embedding(input_dim=10000, output_dim=128, input_length=100)\n",
    "        self.lstm = layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2)\n",
    "        self.quantum_layer = layers.Lambda(lambda x: quantum_layer(x))  # Adding quantum layer\n",
    "        self.dense = layers.Dense(1, activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.lstm(x)\n",
    "        q_output = self.quantum_layer(x)  # Quantum processing step\n",
    "        x = tf.concat([x, q_output], axis=-1)  # Combining classical and quantum outputs\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "# Preprocess Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build and compile the QDL model\n",
    "qdl_model = QDLModel(num_features=100)\n",
    "qdl_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the QDL model\n",
    "qdl_model.fit(X_train, y_train, epochs=2, batch_size=128, validation_split=0.1)\n",
    "\n",
    "# Evaluate the QDL model\n",
    "loss, qdl_acc = qdl_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nQDL (Quantum-Deep Learning) Accuracy: {qdl_acc:.4f}\")\n",
    "\n",
    "# Save the QDL model\n",
    "qdl_model.save(\"best_qdl_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
